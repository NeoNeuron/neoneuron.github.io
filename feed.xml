<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://neoneuron.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://neoneuron.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-01T19:48:05+00:00</updated><id>https://neoneuron.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Kai Chen. Here, you may find its ideas about life, research, and technical stuffs. ENJOY the life! </subtitle><entry><title type="html">Compiling boost using gcc on macOS</title><link href="https://neoneuron.github.io/blog/2023/compile-boost/" rel="alternate" type="text/html" title="Compiling boost using gcc on macOS"/><published>2023-08-28T00:00:00+00:00</published><updated>2023-08-28T00:00:00+00:00</updated><id>https://neoneuron.github.io/blog/2023/compile-boost</id><content type="html" xml:base="https://neoneuron.github.io/blog/2023/compile-boost/"><![CDATA[<p>Redirect to external notion page.</p>]]></content><author><name></name></author><category term="code"/><category term="C/C++"/><category term="Boost"/><summary type="html"><![CDATA[Redirect to external notion page.]]></summary></entry><entry><title type="html">Guide to install DeepDendrite</title><link href="https://neoneuron.github.io/blog/2023/install-deepdendrite/" rel="alternate" type="text/html" title="Guide to install DeepDendrite"/><published>2023-08-18T00:00:00+00:00</published><updated>2023-08-18T00:00:00+00:00</updated><id>https://neoneuron.github.io/blog/2023/install-deepdendrite</id><content type="html" xml:base="https://neoneuron.github.io/blog/2023/install-deepdendrite/"><![CDATA[<p>Redirect to external notion page.</p>]]></content><author><name></name></author><category term="code"/><category term="C/C++"/><category term="Python"/><summary type="html"><![CDATA[Redirect to external notion page.]]></summary></entry><entry><title type="html">Fokker Planck Equation for LIF Neuronal Networks</title><link href="https://neoneuron.github.io/blog/2023/FPE-and-NMM/" rel="alternate" type="text/html" title="Fokker Planck Equation for LIF Neuronal Networks"/><published>2023-02-10T00:00:00+00:00</published><updated>2023-02-10T00:00:00+00:00</updated><id>https://neoneuron.github.io/blog/2023/FPE-and-NMM</id><content type="html" xml:base="https://neoneuron.github.io/blog/2023/FPE-and-NMM/"><![CDATA[<div class="l-page-outset"> 给定$N$个兴奋性LIF神经元，将他们依照一个固定的概率$p$随机链接构成一个网络，其中每个神经元的细胞跨膜电压可以用如下方程描述： $$ \begin{equation} \begin{aligned} \frac{dV_i}{dt} = -\frac{g_l}{C_m}(V_i-V_l) + \frac{I_\mathrm{ext}}{C_m} + \sum_j^N\sum_k w_{ij}\delta(t-t_j^k), \end{aligned} \label{eq:lif} \end{equation} $$ 其中$g_l$，$C_m$，$V_l$分别为神经元的电导，电容和漏电电位，$I_\mathrm{ext}$为外部输入电流，$w_{ij}$为神经元$i$和$j$之间的作用强度，即神经元$j$接收到来自神经元$i$的一个脉冲输入后电压的将上升$w_{ij}$，而$t_j^k$为神经元$j$的第$k$次脉冲发放时间。下面我们假设，网络中所有的神经元都是接受相同的电流输入，且不同神经元之间的作用强度$w_{ij}= w$，均相同。由于所有神经元在网络中的地位都是相同的，与其通过$N$个相互耦合的常微分方程来刻画网络中所有神经元在任意$t$时刻的电压状态$V_i$，不如统计一下每个$t$时刻处于不同电压值的神经元的数量。若将LIF神经元的静息电位$V_\text{r}$到阈值电位$V_\theta$，平均分分成长度为$\Delta V$的等间隔小区间，统计$t$时刻电压值落在$[V-\Delta V/2,V+\Delta V/2)$区间内的神经元个数，$n(V,t)$，则落在该区间内的神经元数量的概率为： $$ \begin{equation} P(V,t) = \frac{n(V,t)}{N}. \end{equation} $$ 根据最朴素的神经元数目守恒的原则，我们下面简单分析一下，在$t+\Delta t$时刻，$P(V,t+\Delta t)$将会是多少。神经元膜电位的变化由\ref{eq:lif}决定，因此区间$[V-\Delta V/2, V+\Delta V/2)$内神经元数目的增加由三个部分贡献。第一，从$[V-3\Delta V/2, V-\Delta V/2)$转移到$[V-\Delta V/2, V+\Delta V/2)$的神经元数量，由外加电流$I_\mathrm{ext}$的强度决定，可以由如下方程描述： $$ \begin{equation} P(V-\Delta V, t)\frac{I_\mathrm{ext}}{C_m}\Delta t. \end{equation} $$ 第二，从$[V-\Delta V/2, V+\Delta V/2)$转移到$[V+\Delta V/2, V+3\Delta V/2)$的神经元数量，由神经元漏电流的强度与当前神经元的膜电位决定，可以由如下方程描述： $$ \begin{equation} P(V+\Delta V, t)\frac{g_l}{C_m}(V+\Delta V-V_l)\Delta t. \end{equation} $$ 第三，其他神经元放电所产生的突触电流，公式\ref{eq:lif}中的等式右端第三项。由于网络中神经元的发放时间是随机的，不妨将其视为平均放电率相同的Poisson过程。由此，我们统计平均单位时间内每个神经元接收到的突触发放电流数目为$\nu(t)$，其中 $$ \nu(t) = \frac{1}{\Delta}\int\limits_{t-\Delta t/2}^{t+\Delta t/2}\sum_{k=1}^N\delta(\tau-t_k)d\tau. $$ 由于突触后膜电位在接收到输入后会瞬时增加$w$，因此，突触输入将使得$[V-\Delta V/2, V+\Delta V/2)$区间内神经元数目增加： $$ \begin{equation} \nu(t)\Delta t \cdot P(V-w, t)\cdot \Delta V. \end{equation} $$ 类似的，从$[V-\Delta V/2, V+\Delta V/2]$流出的神经元数量，如下方程描述： $$ \begin{equation} \begin{aligned} \left[V-\frac{\Delta V}{2}, V+\frac{\Delta V}{2}\right)\to\left[V-\frac{3\Delta V}{2}, V-\frac{\Delta V}{2}\right): &amp;\quad P(V, t)\frac{g_l}{C_m}(V-V_l)\Delta t,\\ \left[V-\frac{\Delta V}{2}, V+\frac{\Delta V}{2}\right)\to\left[V+\frac{\Delta V}{2}, V+\frac{3\Delta V}{2}\right): &amp;\quad P(V, t)\frac{I_\mathrm{ext}}{C_m}\Delta t,\\ \left[V-\frac{\Delta V}{2}, V+\frac{\Delta V}{2}\right)\to\left[V+w-\frac{\Delta V}{2}, V+w+\frac{\Delta V}{2}\right): &amp;\quad \nu(t)\Delta t \cdot P(V, t)\Delta V.\\ \end{aligned} \end{equation} $$ <div class="row"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/FPE-480.webp 480w,/assets/figures/FPE-800.webp 800w,/assets/figures/FPE-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/FPE.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p style="text-align:center"> <b>主方程</b>。主要描述位于膜电位区间$\left[V-\Delta V/2,V+\Delta V/2\right]$中神经元数目的概率密度在$t$时刻的变化情况。</p> 因此，我们可以得到如下方程描述的神经元数目的变化： $$ \begin{equation} \begin{aligned} \left[P(V, t+\Delta t) - P(V, t)\right]\Delta V &amp;= \text{流入区间的神经元数目}-\text{流出区间的神经元数目}\\ &amp;=P(V-\Delta V, t)\frac{I_\mathrm{ext}}{C_m}\Delta t+ P(V+\Delta V, t)\frac{g_l}{C_m}(V+\Delta V-V_l)\Delta t + \nu(t)\Delta t \cdot P(V-w, t)\Delta V \\ &amp;\quad- P(V, t)\frac{g_l}{C_m}(V-V_l)\Delta t - P(V, t)\frac{I_\mathrm{ext}}{C_m}\Delta t-\nu(t)\Delta t \cdot P(V, t)\Delta V\\ &amp;=\left[P(V-\Delta V, t)-P(V,t)\right]\frac{I_\mathrm{ext}}{C_m}\Delta t+ \left[P(V+\Delta V, t)f(V+\Delta V)-P(V,t)f(V)\right]\Delta t \\ &amp;\quad+ \nu(t)\Delta t \Delta V \cdot \left[P(V-w, t) - P(V, t)\right] \end{aligned} \end{equation} $$ 其中$f(V) =g_l(V-V_l)/C_m$。移项整理得到 $$ \begin{equation} \begin{aligned} \frac{\left[P(V, t+\Delta t) - P(V, t)\right]}{\Delta t} &amp;=\frac{\left[P(V-\Delta V, t)-P(V,t)\right]}{\Delta V}\frac{I_\mathrm{ext}}{C_m}+ \frac{\left[P(V+\Delta V, t)f(V+\Delta V)-P(V,t)f(V)\right]}{\Delta V} \\ &amp;\quad+ \nu(t) \cdot \left[P(V-w, t) - P(V, t)\right]. \end{aligned} \label{equ:master} \end{equation} $$ 至此，我们得到了对于电压$V$取值范围内有限离散划分与时间不长$\Delta t$下的主方程(Master equation)。而神经元膜电压是一个连续取值的变量，因此，我们可以讲方程\ref{equ:master}推广到连续形式，即当神经元数目充分大，且电压划分足够密时，$N\to\infty$，$\Delta V\to 0$，我们可以将方程\ref{equ:master}中的$P(V, t)$渐进收敛为膜电压的概率密度函数$p(V,t)$。同时，我们令时间不长$\Delta t\to 0$，则方程\ref{equ:master}中的离散差分变为微分，即可的： $$ \begin{equation} \frac{\partial p(V, t)}{\partial t} = \frac{\partial}{\partial V} \left[\left(f(V)-\frac{I_\mathrm{ext}}{C_m}\right)p(V, t)\right]+ \nu(t) \cdot \left[p(V-w, t) - p(V, t)\right]. \label{equ:master_dev} \end{equation} $$ 方程\ref{equ:master_dev}描述了膜电压概率密度函数的微分方程。其中等式右端第一项为描述了神经元漏电流以及外界连续电流输入对于$V$的分布额影响，第二项描述了网络内神经元随机放电并相互作用所产生的对于膜电压的影响。另外，由于大脑皮层中神经元之间往往存在成百上千个突触连接，而单个突触单个脉冲放电引起的突触后膜电位改变(EPSP/IPSP)，模型中的$w$，相较于膜电位的取值范围，可以视为小量。因此，我们可以运用扩散近似，关于$w$为小量，泰勒展开方程\ref{equ:master_dev}中$P(V-w,t)$右端第二项至二阶，即 $$ \begin{equation} p(V-w, t) = p(V, t) - \frac{\partial p(V, t)}{\partial V} w + \frac{1}{2} \frac{\partial^2 p(V, t)}{\partial V^2} w^2 + \mathcal{O}(w^3) \end{equation} $$ 将上式带入方程\ref{equ:master_dev}并省略三阶小量$\mathcal{O}(w^3)$，得到 $$ \begin{equation} \frac{\partial p(V, t)}{\partial t} = \frac{\partial}{\partial V} \left[\left(f(V)-\frac{I_\mathrm{ext}}{C_m}-\nu(t)w\right)p(V, t)\right]+ \frac{\nu(t)w^2}{2} \frac{\partial^2}{\partial V^2} p(V, t). \label{equ:fpe} \end{equation} $$ 方程\ref{equ:fpe}便是著名的Fokker-Planck方程(FPE)的一维形式。最早由Adriaan Fokker和Max Planck于1914和1917年提出，用于描述统计物理中，流体粒子在随机力和牵引力的作用下，粒子数概率密度函数在平衡状态附近演化的偏微分方程<d-cite key="fokker1914mittlere,risken1996fokkerplanck"></d-cite>。而在神经科学领域，FPE被用于描述神经元膜电位$V$的演化<d-cite key="deco2008dynamic,breakspear2017dynamic"></d-cite>。FPE方程右端第一项为描述了神经元漏电流，外界连续电流输入，以及网络内神经元相互作用的平均效应对于$V$的分布额影响，通常称之为对流项。第二项描述了网络内神经元随机放电的相互作用所产生的对于膜电压涨落的影响，称之为扩散项。 对于LIF神经元模型，即便在扩散近似的极限条件下，$p(V,t)$仍然存在不连续的情况，即当$V=V_\theta$时，神经元将产生动作电位并重置电压值为$V_\mathrm{r}$。因此，$p(V_\theta,t)$存在神经元数目减少，其速率为神经元的平均放电率，即$\left.\frac{\partial p}{\partial t}\right|_{V=V_\theta}=-\left.\frac{\partial \boldsymbol{J}}{\partial V}\right|_{V=V_\theta}+A(t)$，其中，$A(t)$为神经元的平均放电率。与此同时，由于膜电位重置的机制，将有等量的神经元在$V=V_\mathrm{r}$处补充。因此，我们需要方程\ref{equ:fpe}进行修正，添加LIF发放重置机制对于$p(V,t)$的影响： $$ \begin{equation} \begin{aligned} \frac{\partial p(V, t)}{\partial t} &amp;= \frac{\partial}{\partial V} \left[\left(f(V)-\frac{I_\mathrm{ext}}{C_m}-\nu(t)w\right)p(V, t)\right]+ \frac{\nu(t)w^2}{2} \frac{\partial^2}{\partial V^2} p(V, t)\\ &amp;\quad + A(t) \delta\left(V-V_\mathrm{r}\right) - A(t)\delta\left(V-V_\theta\right). \end{aligned} \label{equ:fpe_lif} \end{equation} $$ 根据LIF神经元的发放重置机制，易知LIF神经元的膜电位不会超过$V_\theta$，即当$V&gt;V_\theta$时，$p(V,t)=0$。另外，根据扩散极限的限制，我们可以得知当$V=V_\theta$时，$p(V,t)=0$。因此，根据上述边界条件并结合适当的初值条件，我们可以通过解析或数值方法求解方程\ref{equ:fpe_lif}，得到膜电压概率密度函数$p(V,t)$，神经元平均放电率$A(t)$的解。 根据上文的讨论，我们发现FPE将神经网络包含$\mathcal{O}(N)$自由度的庞大高维动力系统简化成一个仅包含两个变量的一维偏微分方程系统。这种模型简化大大降低了模型的复杂度，让我们可以复杂的网络LIF网络动力学有了直观的理解。同时，我们注意到上述FPE的基于两个简单假设：一，网络内神经元全为兴奋性的基于电流的LIF神经元；二，神经元之间的连接权重相同，拓扑上随机连接，没有空间结构的差异。为了更好的符合皮层真实神经元网络的特点，我们需要对于更一般的FPE模型，例如引入基于电导的神经元模型<d-cite key="cai2006kinetica"></d-cite>，考虑小世界网络或无标度网络等更符合生物神经网络特点的网络连接结构等。 </div>]]></content><author><name>Kai Chen</name></author><category term="neuroscience"/><category term="Fokker-Planck"/><summary type="html"><![CDATA[给定$N$个兴奋性LIF神经元，将他们依照一个固定的概率$p$随机链接构成一个网络，其中每个神经元的细胞跨膜电压可以用如下方程描述： $$ \begin{equation} \begin{aligned} \frac{dV_i}{dt} = -\frac{g_l}{C_m}(V_i-V_l) + \frac{I_\mathrm{ext}}{C_m} + \sum_j^N\sum_k w_{ij}\delta(t-t_j^k), \end{aligned} \label{eq:lif} \end{equation} $$ 其中$g_l$，$C_m$，$V_l$分别为神经元的电导，电容和漏电电位，$I_\mathrm{ext}$为外部输入电流，$w_{ij}$为神经元$i$和$j$之间的作用强度，即神经元$j$接收到来自神经元$i$的一个脉冲输入后电压的将上升$w_{ij}$，而$t_j^k$为神经元$j$的第$k$次脉冲发放时间。下面我们假设，网络中所有的神经元都是接受相同的电流输入，且不同神经元之间的作用强度$w_{ij}= w$，均相同。由于所有神经元在网络中的地位都是相同的，与其通过$N$个相互耦合的常微分方程来刻画网络中所有神经元在任意$t$时刻的电压状态$V_i$，不如统计一下每个$t$时刻处于不同电压值的神经元的数量。若将LIF神经元的静息电位$V_\text{r}$到阈值电位$V_\theta$，平均分分成长度为$\Delta V$的等间隔小区间，统计$t$时刻电压值落在$[V-\Delta V/2,V+\Delta V/2)$区间内的神经元个数，$n(V,t)$，则落在该区间内的神经元数量的概率为： $$ \begin{equation} P(V,t) = \frac{n(V,t)}{N}. \end{equation} $$ 根据最朴素的神经元数目守恒的原则，我们下面简单分析一下，在$t+\Delta t$时刻，$P(V,t+\Delta t)$将会是多少。神经元膜电位的变化由\ref{eq:lif}决定，因此区间$[V-\Delta V/2, V+\Delta V/2)$内神经元数目的增加由三个部分贡献。第一，从$[V-3\Delta V/2, V-\Delta V/2)$转移到$[V-\Delta V/2, V+\Delta V/2)$的神经元数量，由外加电流$I_\mathrm{ext}$的强度决定，可以由如下方程描述： $$ \begin{equation} P(V-\Delta V, t)\frac{I_\mathrm{ext}}{C_m}\Delta t. \end{equation} $$]]></summary></entry><entry><title type="html">The role of population structure in computations through neural dynamics</title><link href="https://neoneuron.github.io/blog/2022/KaiChen1/" rel="alternate" type="text/html" title="The role of population structure in computations through neural dynamics"/><published>2022-02-23T19:00:00+00:00</published><updated>2022-02-23T19:00:00+00:00</updated><id>https://neoneuron.github.io/blog/2022/KaiChen1</id><content type="html" xml:base="https://neoneuron.github.io/blog/2022/KaiChen1/"><![CDATA[<p>It has been widely recorded that the population neuronal activities pocess the low dimensional manifold in multiple brain regions, especially PFC. The origin of low dimensional dynamics and the relation between the dynamical properties and the network structures remain open questions. One of the potential solutions is that low dimensional dynamics is generated by low-rank network architectures. This work trained low-rank recurrent neural networks to perform 5 distinct cognitive tasks respectively, and theoretically analyzed the network dynamics performing computation for each task. Their work showed that very few ranks (1-2) of network structure are actually required to well perform those cognitive tasks. For those tasks with flexible input-target mapping, multiple cell-types (sub-populations) are necessary to perform tasks. Overall, their theory of low-rank RNN can extract the effective latent dynamics for computation, and furthermore provide a framework to networks with multitasking ability.</p> <div class="row"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2022-02-23-480.webp 480w,/assets/img/2022-02-23-800.webp 800w,/assets/img/2022-02-23-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2022-02-23.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div>]]></content><author><name>Kai Chen</name></author><category term="Journal Club"/><category term="RNN"/><category term="low-rank-network"/><category term="dynamical-system"/><category term="computation"/><category term="global-structure"/><category term="gating-mechanism"/><summary type="html"><![CDATA[It has been widely recorded that the population neuronal activities pocess the low dimensional manifold in multiple brain regions, especially PFC. The origin of low dimensional dynamics and the relation between the dynamical properties and the network structures remain open questions. One of the potential solutions is that low dimensional dynamics is generated by low-rank network architectures. This work trained low-rank recurrent neural networks to perform 5 distinct cognitive tasks respectively, and theoretically analyzed the network dynamics performing computation for each task. Their work showed that very few ranks (1-2) of network structure are actually required to well perform those cognitive tasks. For those tasks with flexible input-target mapping, multiple cell-types (sub-populations) are necessary to perform tasks. Overall, their theory of low-rank RNN can extract the effective latent dynamics for computation, and furthermore provide a framework to networks with multitasking ability.]]></summary></entry><entry><title type="html">Context-dependent computation by recurrent dynamics in prefrontal cortex</title><link href="https://neoneuron.github.io/blog/2021/KaiChen4/" rel="alternate" type="text/html" title="Context-dependent computation by recurrent dynamics in prefrontal cortex"/><published>2021-12-13T19:00:00+00:00</published><updated>2021-12-13T19:00:00+00:00</updated><id>https://neoneuron.github.io/blog/2021/KaiChen4</id><content type="html" xml:base="https://neoneuron.github.io/blog/2021/KaiChen4/"><![CDATA[<p>Mante et. al. showed how neurons with complex response coordinate together to do computations of selective integrations in monkey PFC. They trained an siRNN to model the psychophysical behavior of monkeys. By analyzing the modeled siRNN using theory of linear dynamical system, the response of siRNN fits almost perfectly with monkey data in the population level. Furthermore, siRNN produced a novel mechanism to unify selection and integration in a single circuit in terms of line attractor and selection vector.</p>]]></content><author><name>Kai Chen</name></author><category term="Journal Club"/><category term="RNN"/><category term="fixed-point-analysis"/><category term="dynamical-system"/><category term="computation"/><summary type="html"><![CDATA[Mante et. al. showed how neurons with complex response coordinate together to do computations of selective integrations in monkey PFC. They trained an siRNN to model the psychophysical behavior of monkeys. By analyzing the modeled siRNN using theory of linear dynamical system, the response of siRNN fits almost perfectly with monkey data in the population level. Furthermore, siRNN produced a novel mechanism to unify selection and integration in a single circuit in terms of line attractor and selection vector.]]></summary></entry><entry><title type="html">NESTML Installation</title><link href="https://neoneuron.github.io/blog/2021/install-nestml/" rel="alternate" type="text/html" title="NESTML Installation"/><published>2021-11-24T00:00:00+00:00</published><updated>2021-11-24T00:00:00+00:00</updated><id>https://neoneuron.github.io/blog/2021/install-nestml</id><content type="html" xml:base="https://neoneuron.github.io/blog/2021/install-nestml/"><![CDATA[<h2 id="get-started-with-nest-and-nestml">Get started with <code class="language-plaintext highlighter-rouge">NEST</code> and <code class="language-plaintext highlighter-rouge">NESTML</code></h2> <h3 id="install-latest-version-of-cmake-need-sudo-authorization">Install latest version of <code class="language-plaintext highlighter-rouge">cmake</code> (need <code class="language-plaintext highlighter-rouge">sudo</code> authorization)</h3> <p><em>Reference: <a href="https://graspingtech.com/upgrade-cmake/">https://graspingtech.com/upgrade-cmake/</a></em></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>build-essential libssl-dev
wget https://github.com/Kitware/CMake/releases/download/v3.22.0/cmake-3.22.0.tar.gz
<span class="nb">tar</span> <span class="nt">-zxvf</span> cmake-3.22.0.tar.gz
<span class="nb">cd </span>cmake-3.22.0
./bootstrap
make 
<span class="nb">sudo </span>make <span class="nb">install</span> 
</code></pre></div></div> <h3 id="install-nest-v31">Install <code class="language-plaintext highlighter-rouge">NEST</code> <code class="language-plaintext highlighter-rouge">v3.1</code></h3> <p><em>Reference: <a href="https://nest-simulator.readthedocs.io/en/stable/installation/linux_install.html">https://nest-simulator.readthedocs.io/en/stable/installation/linux_install.html</a></em></p> <p><strong>WARNING</strong>: Directly install NEST v2.20.1 from <code class="language-plaintext highlighter-rouge">pip</code> or <code class="language-plaintext highlighter-rouge">conda</code> might leads to <a href="https://github.com/nest/nestml/issues/670">this issue</a>.</p> <ul> <li>Create a new Python environment. <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  conda create <span class="nt">-n</span> nestml <span class="nv">python</span><span class="o">=</span>3.9 <span class="nt">-y</span>
</code></pre></div> </div> </li> <li>Install necessary dependence. <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  conda activate nestml
  conda <span class="nb">install</span> <span class="nt">-y</span> numpy cython matplotlib ipython scipy
  conda <span class="nb">install</span> <span class="nt">-y</span> pytest pytest-xdist pytest-timeout
  pip <span class="nb">install </span>junitparser
</code></pre></div> </div> </li> <li>Download <a href="https://github.com/nest/nest-simulator/releases/tag/v3.1">latest NEST package</a> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  wget <span class="nt">-O</span> nest-simulator-3.1.tar.gz https://codeload.github.com/nest/nest-simulator/tar.gz/refs/tags/v3.1
</code></pre></div> </div> </li> <li>Unpack the tarball <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nb">tar</span> <span class="nt">-xzvf</span> nest-simulator-3.1.tar.gz
</code></pre></div> </div> </li> <li>Create a build directory <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nb">mkdir </span>nest-simulator-3.1-build
</code></pre></div> </div> </li> <li>Change to the build directory: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nb">cd </span>nest-simulator-3.1-build
</code></pre></div> </div> </li> <li>Configure NEST. You may need additional cmake options (see <a href="https://nest-simulator.readthedocs.io/en/stable/installation/cmake_options.html">CMake Options for NEST</a>). <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  cmake ../nest-simulator-3.1/
</code></pre></div> </div> </li> <li>Compile and install NEST: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  make
  make <span class="nb">install
  </span>make installcheck
</code></pre></div> </div> </li> </ul> <p>If there is no failure occurs in installcheck, then NEST should now be successfully installed in your active Python environment.</p> <h3 id="install-the-latest-development-version-of-nestml">Install the latest development version of <code class="language-plaintext highlighter-rouge">NEST::ml</code></h3> <p><em>Reference: <a href="https://nest-simulator.readthedocs.io/en/stable/installation/linux_install.html">https://nestml.readthedocs.io/en/latest/installation.html</a></em></p> <p>To obtain the latest development version, clone directly from the master branch of the GitHub repository:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/nest/nestml
</code></pre></div></div> <p>Install into your current Python environment using:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>nestml
python setup.py <span class="nb">install</span> <span class="nt">--user</span>
</code></pre></div></div> <p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">antlr4-python3-runtime==4.10</code> will lead to a <strong>BUG</strong>. <code class="language-plaintext highlighter-rouge">4.9.3</code> is recommended.</p> <h4 id="testing-optional">Testing (Optional)</h4> <p>After installation, correct operation can be tested by:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python setup.py <span class="nb">test</span>
</code></pre></div></div> <p><em>Note that, tests.docstring_comment_test might fail. If so, please ignore it which won’t affect your usage.</em></p> <h3 id="install-example-nestml-file">Install example <code class="language-plaintext highlighter-rouge">*.nestml</code> file</h3> <p><em>Reference: <a href="https://nest-simulator.readthedocs.io/en/stable/installation/linux_install.html">https://nestml.readthedocs.io/en/latest/tutorials/izhikevich/nestml_izhikevich_tutorial.html</a></em></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="n">pynestml.frontend.pynestml_frontend</span> <span class="kn">import</span> <span class="n">to_nest</span><span class="p">,</span> <span class="n">install_nest</span>

<span class="n">NEST_SIMULATOR_INSTALL_LOCATION</span> <span class="o">=</span> <span class="n">nest</span><span class="p">.</span><span class="n">ll_api</span><span class="p">.</span><span class="nf">sli_func</span><span class="p">(</span><span class="sh">"</span><span class="s">statusdict/prefix ::</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">to_nest</span><span class="p">(</span><span class="n">input_path</span><span class="o">=</span><span class="sh">"</span><span class="s">izhikevich_solution.nestml</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># replace with path of your own nestml file
</span>        <span class="n">target_path</span><span class="o">=</span><span class="sh">"</span><span class="s">~/tmp/nestml-component</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">logging_level</span><span class="o">=</span><span class="sh">"</span><span class="s">ERROR</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">install_nest</span><span class="p">(</span><span class="sh">"</span><span class="s">~/tmp/nestml-component</span><span class="sh">"</span><span class="p">,</span> <span class="n">NEST_SIMULATOR_INSTALL_LOCATION</span><span class="p">)</span>

<span class="n">nest</span><span class="p">.</span><span class="nc">Install</span><span class="p">(</span><span class="sh">"</span><span class="s">nestmlmodule</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="instantiate-model-in-nest-simulator-and-run">Instantiate model in NEST Simulator and run</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplt</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">nest</span>

<span class="n">nest</span><span class="p">.</span><span class="nf">set_verbosity</span><span class="p">(</span><span class="sh">"</span><span class="s">M_WARNING</span><span class="sh">"</span><span class="p">)</span>
<span class="n">nest</span><span class="p">.</span><span class="nc">ResetKernel</span><span class="p">()</span>

<span class="n">neuron</span> <span class="o">=</span> <span class="n">nest</span><span class="p">.</span><span class="nc">Create</span><span class="p">(</span><span class="sh">"</span><span class="s">izhikevich_tutorial</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># replace with name of your own model
</span><span class="n">voltmeter</span> <span class="o">=</span> <span class="n">nest</span><span class="p">.</span><span class="nc">Create</span><span class="p">(</span><span class="sh">"</span><span class="s">voltmeter</span><span class="sh">"</span><span class="p">)</span>

<span class="n">voltmeter</span><span class="p">.</span><span class="nf">set</span><span class="p">({</span><span class="sh">"</span><span class="s">record_from</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">v</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">u</span><span class="sh">"</span><span class="p">]})</span>
<span class="n">nest</span><span class="p">.</span><span class="nc">Connect</span><span class="p">(</span><span class="n">voltmeter</span><span class="p">,</span> <span class="n">neuron</span><span class="p">)</span>

<span class="n">cgs</span> <span class="o">=</span> <span class="n">nest</span><span class="p">.</span><span class="nc">Create</span><span class="p">(</span><span class="sh">'</span><span class="s">dc_generator</span><span class="sh">'</span><span class="p">)</span>
<span class="n">cgs</span><span class="p">.</span><span class="nf">set</span><span class="p">({</span><span class="sh">"</span><span class="s">amplitude</span><span class="sh">"</span><span class="p">:</span> <span class="mf">25.</span><span class="p">})</span>
<span class="n">nest</span><span class="p">.</span><span class="nc">Connect</span><span class="p">(</span><span class="n">cgs</span><span class="p">,</span> <span class="n">neuron</span><span class="p">)</span>

<span class="n">sr</span> <span class="o">=</span> <span class="n">nest</span><span class="p">.</span><span class="nc">Create</span><span class="p">(</span><span class="sh">"</span><span class="s">spike_recorder</span><span class="sh">"</span><span class="p">)</span>
<span class="n">nest</span><span class="p">.</span><span class="nc">Connect</span><span class="p">(</span><span class="n">neuron</span><span class="p">,</span> <span class="n">sr</span><span class="p">)</span>

<span class="n">nest</span><span class="p">.</span><span class="nc">Simulate</span><span class="p">(</span><span class="mf">250.</span><span class="p">)</span>

<span class="n">spike_times</span> <span class="o">=</span> <span class="n">nest</span><span class="p">.</span><span class="nc">GetStatus</span><span class="p">(</span><span class="n">sr</span><span class="p">,</span> <span class="n">keys</span><span class="o">=</span><span class="sh">'</span><span class="s">events</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">times</span><span class="sh">'</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">voltmeter</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">events</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">times</span><span class="sh">"</span><span class="p">],</span> <span class="n">voltmeter</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">events</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">v</span><span class="sh">"</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">voltmeter</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">events</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">times</span><span class="sh">"</span><span class="p">],</span> <span class="n">voltmeter</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">events</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">u</span><span class="sh">"</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">spike_times</span><span class="p">,</span> <span class="mi">30</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">spike_times</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">d</span><span class="sh">"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">99</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_ax</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">_ax</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">v [mV]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">u</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time [ms]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">test.png</span><span class="sh">'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/figures/2021-11-24-test-480.webp 480w,/assets/figures/2021-11-24-test-800.webp 800w,/assets/figures/2021-11-24-test-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/figures/2021-11-24-test.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div>]]></content><author><name></name></author><category term="code"/><category term="NEST-simulator"/><summary type="html"><![CDATA[Get started with NEST and NESTML]]></summary></entry><entry><title type="html">Learning function from structure in neuromorphic networks</title><link href="https://neoneuron.github.io/blog/2021/KaiChen3/" rel="alternate" type="text/html" title="Learning function from structure in neuromorphic networks"/><published>2021-09-08T19:00:00+00:00</published><updated>2021-09-08T19:00:00+00:00</updated><id>https://neoneuron.github.io/blog/2021/KaiChen3</id><content type="html" xml:base="https://neoneuron.github.io/blog/2021/KaiChen3/"><![CDATA[<p>Human brain can perform various of cognitive tasks and is able to flexibly learn new tasks without interfering other tasks. Whether and how the learning and computing capability is inherited from the brain connectomes remains unknown. This work tried to link the learning function and brain connectome in the framework of reservoir computing. They showed that the brain connectome outperform random network at crtical dynamical regime. Futhermore, they found that functional parcellation helps regulate the information flow which might facilitate the cognitive computation in brain</p>]]></content><author><name>Kai Chen</name></author><category term="Journal Club"/><category term="RNN"/><category term="reservoir-computing"/><category term="ESN"/><category term="dynamical-system"/><category term="computation"/><category term="memory-capacity"/><summary type="html"><![CDATA[Human brain can perform various of cognitive tasks and is able to flexibly learn new tasks without interfering other tasks. Whether and how the learning and computing capability is inherited from the brain connectomes remains unknown. This work tried to link the learning function and brain connectome in the framework of reservoir computing. They showed that the brain connectome outperform random network at crtical dynamical regime. Futhermore, they found that functional parcellation helps regulate the information flow which might facilitate the cognitive computation in brain]]></summary></entry><entry><title type="html">One Step Back, Two Steps Forward: Interference and Learning in Recurrent Neural Networks</title><link href="https://neoneuron.github.io/blog/2021/KaiChen2/" rel="alternate" type="text/html" title="One Step Back, Two Steps Forward: Interference and Learning in Recurrent Neural Networks"/><published>2021-05-20T19:00:00+00:00</published><updated>2021-05-20T19:00:00+00:00</updated><id>https://neoneuron.github.io/blog/2021/KaiChen2</id><content type="html" xml:base="https://neoneuron.github.io/blog/2021/KaiChen2/"><![CDATA[<p>Catastrophic forgetting is a key issue in continual learning paradigm. Training algorithms, like FORCE, seem to be able to bypass this to some extent. Chen and Barak applied fixed point analysis to explicitly show the change of fixed point structure of networks during training in continual learning scenario. Their work provide intuitions about how learning algorithm and the order of task sequence affect the training in continual learning.</p> <ul> <li>FORCE: slower convergence in one trial, faster convergence in multi trials.</li> <li>FORCE: less biological plausible, more powerful in sequential learning.</li> <li>LMS (least mean square): smaller learning rate, less interference.</li> </ul>]]></content><author><name>Kai Chen</name></author><category term="Journal Club"/><category term="RNN"/><category term="fixed-point-analysis"/><category term="dynamical-system"/><category term="computation"/><summary type="html"><![CDATA[Catastrophic forgetting is a key issue in continual learning paradigm. Training algorithms, like FORCE, seem to be able to bypass this to some extent. Chen and Barak applied fixed point analysis to explicitly show the change of fixed point structure of networks during training in continual learning scenario. Their work provide intuitions about how learning algorithm and the order of task sequence affect the training in continual learning.]]></summary></entry><entry><title type="html">Universality and individuality in neural dynamics across large populations of recurrent networks</title><link href="https://neoneuron.github.io/blog/2021/KaiChen1/" rel="alternate" type="text/html" title="Universality and individuality in neural dynamics across large populations of recurrent networks"/><published>2021-04-21T19:00:00+00:00</published><updated>2021-04-21T19:00:00+00:00</updated><id>https://neoneuron.github.io/blog/2021/KaiChen1</id><content type="html" xml:base="https://neoneuron.github.io/blog/2021/KaiChen1/"><![CDATA[<p>Multi-solution is a prominant feature of ANNs (DNNs/RNNs) when training to perform certain tasks. Is there any common feature between different solutions remains an open questions. This works found that the topology of fixed points of trained network is the universally shared between different network architectures and realizations when those networks are trained for the same task. Further, they demonstrated the topological structure of fixed points of networks indeed interprets computation mechanism of trained networks.</p> <ul> <li> <p>Networks with different architectures and nonlinearities do differ in the sense of conventional representational similarity analysis (RSA).</p> </li> <li> <p>Fixed point topology can be invariant across networks performing same tasks.</p> </li> <li> <p>Linearized dynamics showed the computational ability in the dynamical regime around the fixed point points, which further proved the common dynamical feature across different network realizations.</p> </li> <li> <p>This RSA protocol on fixed point topology might be useful to quantify the similarity between RNN and BNNs (biological neuronal networks). (Also it is the critical issue when we use DL to study neurophysiological data.)</p> </li> <li> <p>Understanding fixed point topology and linear behavior around “generalized fixed points” can be a good perspective to study the computation through dynamics in RNNs, with related topics as memory capacity, learning dynamics, online learning (continual learning), and etc.</p> </li> </ul>]]></content><author><name>Kai Chen</name></author><category term="Journal Club"/><category term="RNN"/><category term="fixed-point-analysis"/><category term="dynamical-system"/><category term="computation"/><summary type="html"><![CDATA[Multi-solution is a prominant feature of ANNs (DNNs/RNNs) when training to perform certain tasks. Is there any common feature between different solutions remains an open questions. This works found that the topology of fixed points of trained network is the universally shared between different network architectures and realizations when those networks are trained for the same task. Further, they demonstrated the topological structure of fixed points of networks indeed interprets computation mechanism of trained networks.]]></summary></entry><entry><title type="html">Set Router Bridge Mode</title><link href="https://neoneuron.github.io/blog/2021/router-bridge/" rel="alternate" type="text/html" title="Set Router Bridge Mode"/><published>2021-01-30T00:00:00+00:00</published><updated>2021-01-30T00:00:00+00:00</updated><id>https://neoneuron.github.io/blog/2021/router-bridge</id><content type="html" xml:base="https://neoneuron.github.io/blog/2021/router-bridge/"><![CDATA[<p>This is a note for router bridge mode setups.</p> <h2 id="setup-main-router">Setup main router</h2> <p>Follow the default instruction of router setups.</p> <h2 id="setup-bridging-router">Setup bridging router</h2> <ol> <li>Connect the WIFI of router, and open <code class="language-plaintext highlighter-rouge">192.168.1.1</code> from your browser.</li> <li>Set the LAN IP address to <code class="language-plaintext highlighter-rouge">192.168.1.100</code>, and restart the router.</li> <li>Open the configuration website with new IP <code class="language-plaintext highlighter-rouge">192.168.1.100</code>.</li> <li>Turn off the DHCP server.</li> <li>Set a proper range for address pool. Note that it should not cover the LAN IP address you set previously.</li> <li>Go to wireless setting to open WDS. Scan the AP list and select the main router.</li> <li>Enter the password to bridge to the main router.</li> <li>Save all settings and restart the router. <em>Recommended:choose the same channel for bridge router as the main ones.</em></li> <li>Reconnect to the bridge router.</li> </ol> <p><strong>ENJOY your day!</strong></p>]]></content><author><name></name></author><category term="tech"/><category term="router-setting"/><summary type="html"><![CDATA[This is a note for router bridge mode setups.]]></summary></entry></feed>